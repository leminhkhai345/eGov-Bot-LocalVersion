{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Code chạy trên Google Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1692,
     "status": "ok",
     "timestamp": 1755705216445,
     "user": {
      "displayName": "Shinran KCC",
      "userId": "14808149736541007016"
     },
     "user_tz": -420
    },
    "id": "mnp8-yyFJE_Z",
    "outputId": "3a0022a3-1989-4b48-9b25-e6fd6e97c776"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22329,
     "status": "ok",
     "timestamp": 1755705238786,
     "user": {
      "displayName": "Shinran KCC",
      "userId": "14808149736541007016"
     },
     "user_tz": -420
    },
    "id": "r-nPlz7PJOj8",
    "outputId": "ced77a9f-a1de-4b4b-bf5f-e2100aa7cca1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ijson in /usr/local/lib/python3.12/dist-packages (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0) CÀI THƯ VIỆN\n",
    "# =========================\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install \"sentence-transformers>=3.0.0\" faiss-cpu \"rank_bm25>=0.2.2\" datasets\n",
    "!pip -q install \"transformers>=4.41.0\" \"accelerate>=0.30.0\"\n",
    "!pip -q install fastapi uvicorn nest_asyncio\n",
    "!pip install ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 21887,
     "status": "ok",
     "timestamp": 1755705260679,
     "user": {
      "displayName": "Shinran KCC",
      "userId": "14808149736541007016"
     },
     "user_tz": -420
    },
    "id": "RjkMXP0xJO5b"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) CẤU HÌNH & IMPORT\n",
    "# =========================\n",
    "import os, json, math, gzip, pickle, textwrap, re, uuid, time\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# --- Đường dẫn làm việc trên Colab ---\n",
    "WORK_DIR = \"/content/drive/MyDrive/eGov-Bot/ITB\"\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "RAW_JSON_PATH   = f\"{WORK_DIR}/toan_bo_du_lieu.json\"      # dữ liệu gốc (list các thủ tục)\n",
    "CHUNKS_JSONL    = f\"{WORK_DIR}/chunks.jsonl\"  # dữ liệu đã chunk (mỗi dòng 1 chunk)\n",
    "FAISS_INDEX     = f\"{WORK_DIR}/index.faiss\"   # file chỉ mục FAISS\n",
    "METAS_PKL_GZ    = f\"{WORK_DIR}/metas.pkl.gz\"  # metadata kèm text\n",
    "BM25_PICKLE     = f\"{WORK_DIR}/bm25.pkl.gz\"   # bm25 corpus (tuỳ chọn)\n",
    "\n",
    "# --- Model embedding & reranker ---\n",
    "EMBED_MODEL_NAME   = \"AITeamVN/Vietnamese_Embedding\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zb-5pbQ9Kaxx",
    "outputId": "cb7822b2-f866-4b66-d6ba-c655d837eea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ROBUST JSON CHUNKING ===\n",
      "=== BẮT ĐẦU CHUNKING (MULTICORE) ===\n",
      "📁 Kích thước file: 70.24 MB\n",
      "🖥️ Phát hiện 2 CPU core – dùng tối đa\n",
      "🔄 Đang parse JSON stream...\n",
      "📖 Đã đọc 400000 ký tự...\n",
      "✅ 100 records → 1023 chunks\n",
      "📖 Đã đọc 600000 ký tự...\n",
      "✅ 200 records → 2003 chunks\n",
      "✅ 300 records → 2970 chunks\n",
      "✅ 400 records → 3939 chunks\n",
      "✅ 500 records → 5151 chunks\n",
      "✅ 600 records → 6190 chunks\n",
      "✅ 700 records → 7233 chunks\n",
      "✅ 800 records → 8299 chunks\n",
      "✅ 900 records → 9375 chunks\n",
      "✅ 1000 records → 10358 chunks\n",
      "✅ 1100 records → 11580 chunks\n",
      "✅ 1200 records → 12596 chunks\n",
      "📖 Đã đọc 5300000 ký tự...\n",
      "✅ 1300 records → 13613 chunks\n",
      "✅ 1400 records → 14614 chunks\n",
      "✅ 1500 records → 15551 chunks\n",
      "📖 Đã đọc 6600000 ký tự...\n",
      "✅ 1600 records → 16597 chunks\n",
      "✅ 1700 records → 17643 chunks\n",
      "✅ 1800 records → 18674 chunks\n",
      "📖 Đã đọc 7800000 ký tự...\n",
      "✅ 1900 records → 19661 chunks\n",
      "✅ 2000 records → 20723 chunks\n",
      "✅ 2100 records → 21719 chunks\n",
      "📖 Đã đọc 8800000 ký tự...\n",
      "✅ 2200 records → 22998 chunks\n",
      "✅ 2300 records → 24211 chunks\n",
      "✅ 2400 records → 25509 chunks\n",
      "✅ 2500 records → 27251 chunks\n",
      "✅ 2600 records → 28408 chunks\n",
      "✅ 2700 records → 29529 chunks\n",
      "✅ 2800 records → 30609 chunks\n",
      "📖 Đã đọc 12700000 ký tự...\n",
      "✅ 2900 records → 31558 chunks\n",
      "✅ 3000 records → 32623 chunks\n",
      "✅ 3100 records → 33810 chunks\n",
      "✅ 3200 records → 34772 chunks\n",
      "✅ 3300 records → 35746 chunks\n",
      "✅ 3400 records → 36822 chunks\n",
      "✅ 3500 records → 37846 chunks\n",
      "✅ 3600 records → 38960 chunks\n",
      "✅ 3700 records → 40110 chunks\n",
      "✅ 3800 records → 41155 chunks\n",
      "✅ 3900 records → 42278 chunks\n",
      "✅ 4000 records → 43398 chunks\n",
      "✅ 4100 records → 44435 chunks\n",
      "✅ 4200 records → 45595 chunks\n",
      "✅ 4300 records → 46674 chunks\n",
      "✅ 4400 records → 47572 chunks\n",
      "✅ 4500 records → 48870 chunks\n",
      "✅ 4600 records → 49972 chunks\n",
      "✅ 4700 records → 51044 chunks\n",
      "✅ 4800 records → 52255 chunks\n",
      "📖 Đã đọc 21800000 ký tự...\n",
      "✅ 4900 records → 53398 chunks\n",
      "✅ 5000 records → 54524 chunks\n",
      "✅ 5100 records → 55606 chunks\n",
      "✅ 5200 records → 56585 chunks\n",
      "✅ 5300 records → 57716 chunks\n",
      "✅ 5400 records → 58659 chunks\n",
      "✅ 5500 records → 59688 chunks\n",
      "✅ 5600 records → 60907 chunks\n",
      "✅ 5700 records → 62389 chunks\n",
      "✅ 5800 records → 63572 chunks\n",
      "✅ 5900 records → 64491 chunks\n",
      "✅ 6000 records → 65818 chunks\n",
      "✅ 6100 records → 66925 chunks\n",
      "✅ 6200 records → 68082 chunks\n",
      "✅ 6300 records → 69284 chunks\n",
      "✅ 6400 records → 70330 chunks\n",
      "✅ 6500 records → 71378 chunks\n",
      "✅ 6600 records → 72384 chunks\n",
      "📖 Đã đọc 30100000 ký tự...\n",
      "✅ 6700 records → 73481 chunks\n",
      "📖 Đã đọc 30300000 ký tự...\n",
      "✅ 6800 records → 74529 chunks\n",
      "✅ 6900 records → 75615 chunks\n",
      "✅ 7000 records → 76607 chunks\n",
      "✅ 7100 records → 77816 chunks\n",
      "📖 Đã đọc 32200000 ký tự...\n",
      "✅ 7200 records → 78803 chunks\n",
      "✅ 7300 records → 80204 chunks\n",
      "✅ 7400 records → 81088 chunks\n",
      "✅ 7500 records → 82014 chunks\n",
      "✅ 7600 records → 83010 chunks\n",
      "✅ 7700 records → 83963 chunks\n",
      "✅ 7800 records → 85092 chunks\n",
      "📖 Đã đọc 35300000 ký tự...\n",
      "✅ 7900 records → 86140 chunks\n",
      "✅ 8000 records → 87262 chunks\n",
      "✅ 8100 records → 88376 chunks\n",
      "✅ 8200 records → 89464 chunks\n",
      "✅ 8300 records → 90324 chunks\n",
      "✅ 8400 records → 91393 chunks\n",
      "✅ 8500 records → 92622 chunks\n",
      "📖 Đã đọc 38100000 ký tự...\n",
      "✅ 8600 records → 94149 chunks\n",
      "✅ 8700 records → 95177 chunks\n",
      "✅ 8800 records → 96218 chunks\n",
      "✅ 8900 records → 97259 chunks\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 3) HÀM TIỀN XỬ LÝ & CHUNKING (ít RAM)\n",
    "# =========================\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "from typing import Generator, Dict, Any, List\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "\n",
    "FIELD_ORDER = [\n",
    "    \"ten_thu_tuc\", \"yeu_cau_dieu_kien\", \"thanh_phan_ho_so\",\n",
    "    \"trinh_tu_thuc_hien\", \"cach_thuc_thuc_hien\",\n",
    "    \"co_quan_thuc_hien\", \"thu_tuc_lien_quan\"\n",
    "]\n",
    "\n",
    "FIELD_VN_NAME = {\n",
    "    \"ten_thu_tuc\": \"Tên thủ tục\",\n",
    "    \"yeu_cau_dieu_kien\": \"Yêu cầu, điều kiện\",\n",
    "    \"thanh_phan_ho_so\": \"Thành phần hồ sơ\",\n",
    "    \"trinh_tu_thuc_hien\": \"Trình tự thực hiện\",\n",
    "    \"cach_thuc_thuc_hien\": \"Cách thức thực hiện\",\n",
    "    \"co_quan_thuc_hien\": \"Cơ quan thực hiện\",\n",
    "    \"thu_tuc_lien_quan\": \"Thủ tục liên quan\"\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Chuẩn hóa text\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = str(s).replace(\"\\r\\n\", \"\\n\").replace(\"\\t\", \" \").strip()\n",
    "    s = re.sub(r\"[ \\u00A0]+\", \" \", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s\n",
    "\n",
    "def split_text_rcts(\n",
    "    text: str,\n",
    "    max_chars: int = 800,\n",
    "    overlap: int = 50,\n",
    "    separators: List[str] = None\n",
    ") -> Generator[str, None, None]:\n",
    "    \"\"\"Recursive Text Character Split (RCTS)\"\"\"\n",
    "    text = normalize_text(text)\n",
    "    if not text:\n",
    "        return\n",
    "\n",
    "    if separators is None:\n",
    "        separators = [\"\\n\\n\", \"\\n\", \". \", \", \", \" \"]\n",
    "\n",
    "    def _split_recursive(txt: str) -> List[str]:\n",
    "        if len(txt) <= max_chars:\n",
    "            return [txt]\n",
    "\n",
    "        # tìm separator phù hợp nhất\n",
    "        for sep in separators:\n",
    "            idx = txt.rfind(sep, 0, max_chars)\n",
    "            if idx != -1 and idx > max_chars * 0.3:  # tránh cắt quá sớm\n",
    "                left = txt[:idx + len(sep)].strip()\n",
    "                right = txt[idx + len(sep):].strip()\n",
    "                return _split_recursive(left) + _split_recursive(right)\n",
    "\n",
    "        # nếu không tìm được -> cắt cứng\n",
    "        left = txt[:max_chars]\n",
    "        right = txt[max_chars:]\n",
    "        return [left] + _split_recursive(right)\n",
    "\n",
    "    chunks = _split_recursive(text)\n",
    "\n",
    "    # thêm overlap giữa các chunk\n",
    "    final_chunks = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i > 0 and overlap > 0:\n",
    "            prefix = chunks[i-1][-overlap:]\n",
    "            chunk = prefix + \" \" + chunk\n",
    "        final_chunks.append(chunk.strip())\n",
    "\n",
    "    return (c for c in final_chunks if c.strip())\n",
    "\n",
    "def create_chunk(record: Dict[str, Any], field: str, piece: str, piece_idx: int) -> str:\n",
    "    \"\"\"Tạo một chunk JSON\"\"\"\n",
    "    parent_id = str(record.get(\"nguon\", str(uuid.uuid4())[:8]))\n",
    "    title = normalize_text(record.get(\"ten_thu_tuc\", \"\"))\n",
    "    source = str(record.get(\"nguon\", \"\"))\n",
    "\n",
    "    field_name = FIELD_VN_NAME.get(field, field)\n",
    "    text_content = f\"Thủ tục: {title}\\nMục: {field_name}\\nNội dung: {piece}\"\n",
    "\n",
    "    chunk = {\n",
    "        \"id\": f\"{parent_id}#{field}#{piece_idx}\",\n",
    "        \"parent_id\": parent_id,\n",
    "        \"ten_thu_tuc\": title,\n",
    "        \"field\": field,\n",
    "        \"text\": text_content,\n",
    "        \"raw\": piece,\n",
    "        \"nguon\": source\n",
    "    }\n",
    "\n",
    "    return json.dumps(chunk, ensure_ascii=False)\n",
    "\n",
    "def process_record_streaming(record: Dict[str, Any], outfile) -> int:\n",
    "    \"\"\"Xử lý một record và ghi trực tiếp ra file\"\"\"\n",
    "    chunk_count = 0\n",
    "\n",
    "    for field in FIELD_ORDER:\n",
    "        raw_value = record.get(field)\n",
    "        if not raw_value:\n",
    "            continue\n",
    "\n",
    "        piece_idx = 0\n",
    "        for piece in split_text_rcts(str(raw_value)):\n",
    "            if not piece.strip():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                chunk_json = create_chunk(record, field, piece, piece_idx)\n",
    "                outfile.write(chunk_json + \"\\n\")\n",
    "                chunk_count += 1\n",
    "                piece_idx += 1\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Lỗi tạo chunk cho field {field}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return chunk_count\n",
    "\n",
    "def safe_json_stream_parser(filepath: str) -> Generator[Dict[str, Any], None, None]:\n",
    "    \"\"\"Parser JSON streaming an toàn hơn\"\"\"\n",
    "\n",
    "    print(\"🔄 Đang parse JSON stream...\")\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8', buffering=8192) as f:\n",
    "            # Đọc và kiểm tra ký tự đầu\n",
    "            first_char = f.read(1)\n",
    "            if first_char != '[':\n",
    "                print(f\"❌ File không bắt đầu bằng '[', mà là '{first_char}'\")\n",
    "                return\n",
    "\n",
    "            # Reset về đầu và bỏ qua '['\n",
    "            f.seek(1)\n",
    "\n",
    "            buffer = \"\"\n",
    "            brace_count = 0\n",
    "            bracket_count = 0\n",
    "            in_string = False\n",
    "            escape_next = False\n",
    "            char_count = 0\n",
    "\n",
    "            while True:\n",
    "                char = f.read(1)\n",
    "                if not char:\n",
    "                    break\n",
    "\n",
    "                char_count += 1\n",
    "\n",
    "                # Xử lý escape characters\n",
    "                if escape_next:\n",
    "                    buffer += char\n",
    "                    escape_next = False\n",
    "                    continue\n",
    "\n",
    "                if char == '\\\\' and in_string:\n",
    "                    buffer += char\n",
    "                    escape_next = True\n",
    "                    continue\n",
    "\n",
    "                # Xử lý strings\n",
    "                if char == '\"':\n",
    "                    in_string = not in_string\n",
    "                    buffer += char\n",
    "                    continue\n",
    "\n",
    "                if in_string:\n",
    "                    buffer += char\n",
    "                    continue\n",
    "\n",
    "                # Xử lý cấu trúc JSON\n",
    "                if char == '{':\n",
    "                    brace_count += 1\n",
    "                    buffer += char\n",
    "                elif char == '}':\n",
    "                    brace_count -= 1\n",
    "                    buffer += char\n",
    "\n",
    "                    # Hoàn thành một object\n",
    "                    if brace_count == 0 and bracket_count == 0:\n",
    "                        try:\n",
    "                            # Clean buffer trước khi parse\n",
    "                            clean_buffer = buffer.strip().rstrip(',')\n",
    "                            if clean_buffer:\n",
    "                                obj = json.loads(clean_buffer)\n",
    "                                yield obj\n",
    "                                del obj\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"❌ JSON decode error tại ký tự {char_count}: {e}\")\n",
    "                            print(f\"   Buffer: {buffer[:100]}...\")\n",
    "\n",
    "                        # Reset buffer\n",
    "                        buffer = \"\"\n",
    "                        gc.collect()\n",
    "\n",
    "                elif char == '[':\n",
    "                    bracket_count += 1\n",
    "                    buffer += char\n",
    "                elif char == ']':\n",
    "                    if bracket_count > 0:\n",
    "                        bracket_count -= 1\n",
    "                        buffer += char\n",
    "                    else:\n",
    "                        # Kết thúc main array\n",
    "                        break\n",
    "                elif char == ',':\n",
    "                    if brace_count == 0 and bracket_count == 0:\n",
    "                        # Dấu phẩy giữa các objects chính\n",
    "                        continue\n",
    "                    else:\n",
    "                        buffer += char\n",
    "                elif char in ' \\t\\n\\r':\n",
    "                    # Whitespace\n",
    "                    if buffer.strip():  # Chỉ thêm nếu buffer không rỗng\n",
    "                        buffer += ' '\n",
    "                else:\n",
    "                    buffer += char\n",
    "\n",
    "                # Progress indicator\n",
    "                if char_count % 100000 == 0:\n",
    "                    print(f\"📖 Đã đọc {char_count} ký tự...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi đọc file: {e}\")\n",
    "\n",
    "def process_one_record(record: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Xử lý 1 record và trả về danh sách JSON string (chunk).\"\"\"\n",
    "    chunks = []\n",
    "    for field in FIELD_ORDER:\n",
    "        raw_value = record.get(field)\n",
    "        if not raw_value:\n",
    "            continue\n",
    "\n",
    "        piece_idx = 0\n",
    "        for piece in split_text_rcts(str(raw_value)):\n",
    "            if not piece.strip():\n",
    "                continue\n",
    "            try:\n",
    "                chunk_json = create_chunk(record, field, piece, piece_idx)\n",
    "                chunks.append(chunk_json)\n",
    "                piece_idx += 1\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Lỗi tạo chunk cho field {field}: {e}\")\n",
    "                continue\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunking_main():\n",
    "    \"\"\"Chunking chính – chạy song song bằng toàn bộ CPU core\"\"\"\n",
    "    print(\"=== BẮT ĐẦU CHUNKING (MULTICORE) ===\")\n",
    "\n",
    "    if not os.path.exists(RAW_JSON_PATH):\n",
    "        print(f\"❌ File không tồn tại: {RAW_JSON_PATH}\")\n",
    "        return\n",
    "\n",
    "    file_size_mb = os.path.getsize(RAW_JSON_PATH) / (1024 * 1024)\n",
    "    print(f\"📁 Kích thước file: {file_size_mb:.2f} MB\")\n",
    "\n",
    "    total_chunks = 0\n",
    "    processed_records = 0\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    print(f\"🖥️ Phát hiện {num_cores} CPU core – dùng tối đa\")\n",
    "\n",
    "    try:\n",
    "        with open(CHUNKS_JSONL, 'w', encoding='utf-8', buffering=1024) as outfile, \\\n",
    "             ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "\n",
    "            futures = []\n",
    "            for record in safe_json_stream_parser(RAW_JSON_PATH):\n",
    "                futures.append(executor.submit(process_one_record, record))\n",
    "                processed_records += 1\n",
    "\n",
    "                # để tránh dồn quá nhiều future -> flush dần\n",
    "                if len(futures) >= 100:\n",
    "                    for f in as_completed(futures):\n",
    "                        chunks = f.result()\n",
    "                        for c in chunks:\n",
    "                            outfile.write(c + \"\\n\")\n",
    "                        total_chunks += len(chunks)\n",
    "                    outfile.flush()\n",
    "                    futures = []\n",
    "                    gc.collect()\n",
    "\n",
    "                    if processed_records % 50 == 0:\n",
    "                        print(f\"✅ {processed_records} records → {total_chunks} chunks\")\n",
    "\n",
    "            # Xử lý nốt các future còn lại\n",
    "            for f in as_completed(futures):\n",
    "                chunks = f.result()\n",
    "                for c in chunks:\n",
    "                    outfile.write(c + \"\\n\")\n",
    "                total_chunks += len(chunks)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi fatal: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n🎉 HOÀN THÀNH!\")\n",
    "    print(f\"📊 Tổng kết: {processed_records} records → {total_chunks} chunks\")\n",
    "    print(f\"💾 Output: {CHUNKS_JSONL}\")\n",
    "\n",
    "def fallback_simple_load():\n",
    "    \"\"\"Phương án dự phòng: load trực tiếp (cho file nhỏ)\"\"\"\n",
    "    print(\"🔄 Thử phương án load trực tiếp...\")\n",
    "\n",
    "    try:\n",
    "        with open(RAW_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        print(f\"✅ Load thành công! Tổng records: {len(data)}\")\n",
    "\n",
    "        total_chunks = 0\n",
    "\n",
    "        with open(CHUNKS_JSONL, 'w', encoding='utf-8') as outfile:\n",
    "            for i, record in enumerate(data):\n",
    "                try:\n",
    "                    chunks_created = process_record_streaming(record, outfile)\n",
    "                    total_chunks += chunks_created\n",
    "\n",
    "                    if (i + 1) % 100 == 0:\n",
    "                        print(f\"✅ {i + 1}/{len(data)} → {total_chunks} chunks\")\n",
    "                        outfile.flush()\n",
    "                        gc.collect()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Lỗi record {i}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        print(f\"🎉 Fallback hoàn thành: {len(data)} records → {total_chunks} chunks\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Fallback thất bại: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"=== ROBUST JSON CHUNKING ===\")\n",
    "\n",
    "    try:\n",
    "        chunking_main()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Phương pháp chính thất bại: {e}\")\n",
    "        print(\"🔄 Thử phương án dự phong...\")\n",
    "        fallback_simple_load()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3C5CbayMMRMt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMS1oF70FgHr6XaJNKtkE6y",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
