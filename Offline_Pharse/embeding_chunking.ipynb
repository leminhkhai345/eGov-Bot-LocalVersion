{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Code ch·∫°y tr√™n Google Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1692,
     "status": "ok",
     "timestamp": 1755705216445,
     "user": {
      "displayName": "Shinran KCC",
      "userId": "14808149736541007016"
     },
     "user_tz": -420
    },
    "id": "mnp8-yyFJE_Z",
    "outputId": "3a0022a3-1989-4b48-9b25-e6fd6e97c776"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22329,
     "status": "ok",
     "timestamp": 1755705238786,
     "user": {
      "displayName": "Shinran KCC",
      "userId": "14808149736541007016"
     },
     "user_tz": -420
    },
    "id": "r-nPlz7PJOj8",
    "outputId": "ced77a9f-a1de-4b4b-bf5f-e2100aa7cca1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ijson in /usr/local/lib/python3.12/dist-packages (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0) C√ÄI TH∆Ø VI·ªÜN\n",
    "# =========================\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install \"sentence-transformers>=3.0.0\" faiss-cpu \"rank_bm25>=0.2.2\" datasets\n",
    "!pip -q install \"transformers>=4.41.0\" \"accelerate>=0.30.0\"\n",
    "!pip -q install fastapi uvicorn nest_asyncio\n",
    "!pip install ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 21887,
     "status": "ok",
     "timestamp": 1755705260679,
     "user": {
      "displayName": "Shinran KCC",
      "userId": "14808149736541007016"
     },
     "user_tz": -420
    },
    "id": "RjkMXP0xJO5b"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) C·∫§U H√åNH & IMPORT\n",
    "# =========================\n",
    "import os, json, math, gzip, pickle, textwrap, re, uuid, time\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# --- ƒê∆∞·ªùng d·∫´n l√†m vi·ªác tr√™n Colab ---\n",
    "WORK_DIR = \"/content/drive/MyDrive/eGov-Bot/ITB\"\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "RAW_JSON_PATH   = f\"{WORK_DIR}/toan_bo_du_lieu.json\"      # d·ªØ li·ªáu g·ªëc (list c√°c th·ªß t·ª•c)\n",
    "CHUNKS_JSONL    = f\"{WORK_DIR}/chunks.jsonl\"  # d·ªØ li·ªáu ƒë√£ chunk (m·ªói d√≤ng 1 chunk)\n",
    "FAISS_INDEX     = f\"{WORK_DIR}/index.faiss\"   # file ch·ªâ m·ª•c FAISS\n",
    "METAS_PKL_GZ    = f\"{WORK_DIR}/metas.pkl.gz\"  # metadata k√®m text\n",
    "BM25_PICKLE     = f\"{WORK_DIR}/bm25.pkl.gz\"   # bm25 corpus (tu·ª≥ ch·ªçn)\n",
    "\n",
    "# --- Model embedding & reranker ---\n",
    "EMBED_MODEL_NAME   = \"AITeamVN/Vietnamese_Embedding\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zb-5pbQ9Kaxx",
    "outputId": "cb7822b2-f866-4b66-d6ba-c655d837eea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ROBUST JSON CHUNKING ===\n",
      "=== B·∫ÆT ƒê·∫¶U CHUNKING (MULTICORE) ===\n",
      "üìÅ K√≠ch th∆∞·ªõc file: 70.24 MB\n",
      "üñ•Ô∏è Ph√°t hi·ªán 2 CPU core ‚Äì d√πng t·ªëi ƒëa\n",
      "üîÑ ƒêang parse JSON stream...\n",
      "üìñ ƒê√£ ƒë·ªçc 400000 k√Ω t·ª±...\n",
      "‚úÖ 100 records ‚Üí 1023 chunks\n",
      "üìñ ƒê√£ ƒë·ªçc 600000 k√Ω t·ª±...\n",
      "‚úÖ 200 records ‚Üí 2003 chunks\n",
      "‚úÖ 300 records ‚Üí 2970 chunks\n",
      "‚úÖ 400 records ‚Üí 3939 chunks\n",
      "‚úÖ 500 records ‚Üí 5151 chunks\n",
      "‚úÖ 600 records ‚Üí 6190 chunks\n",
      "‚úÖ 700 records ‚Üí 7233 chunks\n",
      "‚úÖ 800 records ‚Üí 8299 chunks\n",
      "‚úÖ 900 records ‚Üí 9375 chunks\n",
      "‚úÖ 1000 records ‚Üí 10358 chunks\n",
      "‚úÖ 1100 records ‚Üí 11580 chunks\n",
      "‚úÖ 1200 records ‚Üí 12596 chunks\n",
      "üìñ ƒê√£ ƒë·ªçc 5300000 k√Ω t·ª±...\n",
      "‚úÖ 1300 records ‚Üí 13613 chunks\n",
      "‚úÖ 1400 records ‚Üí 14614 chunks\n",
      "‚úÖ 1500 records ‚Üí 15551 chunks\n",
      "üìñ ƒê√£ ƒë·ªçc 6600000 k√Ω t·ª±...\n",
      "‚úÖ 1600 records ‚Üí 16597 chunks\n",
      "‚úÖ 1700 records ‚Üí 17643 chunks\n",
      "‚úÖ 1800 records ‚Üí 18674 chunks\n",
      "üìñ ƒê√£ ƒë·ªçc 7800000 k√Ω t·ª±...\n",
      "‚úÖ 1900 records ‚Üí 19661 chunks\n",
      "‚úÖ 2000 records ‚Üí 20723 chunks\n",
      "‚úÖ 2100 records ‚Üí 21719 chunks\n",
      "üìñ ƒê√£ ƒë·ªçc 8800000 k√Ω t·ª±...\n",
      "‚úÖ 2200 records ‚Üí 22998 chunks\n",
      "‚úÖ 2300 records ‚Üí 24211 chunks\n",
      "‚úÖ 2400 records ‚Üí 25509 chunks\n",
      "‚úÖ 2500 records ‚Üí 27251 chunks\n",
      "‚úÖ 2600 records ‚Üí 28408 chunks\n",
      "‚úÖ 2700 records ‚Üí 29529 chunks\n",
      "‚úÖ 2800 records ‚Üí 30609 chunks\n",
      "üìñ ƒê√£ ƒë·ªçc 12700000 k√Ω t·ª±...\n",
      "‚úÖ 2900 records ‚Üí 31558 chunks\n",
      "‚úÖ 3000 records ‚Üí 32623 chunks\n",
      "‚úÖ 3100 records ‚Üí 33810 chunks\n",
      "‚úÖ 3200 records ‚Üí 34772 chunks\n",
      "‚úÖ 3300 records ‚Üí 35746 chunks\n",
      "‚úÖ 3400 records ‚Üí 36822 chunks\n",
      "‚úÖ 3500 records ‚Üí 37846 chunks\n",
      "‚úÖ 3600 records ‚Üí 38960 chunks\n",
      "‚úÖ 3700 records ‚Üí 40110 chunks\n",
      "‚úÖ 3800 records ‚Üí 41155 chunks\n",
      "‚úÖ 3900 records ‚Üí 42278 chunks\n",
      "‚úÖ 4000 records ‚Üí 43398 chunks\n",
      "‚úÖ 4100 records ‚Üí 44435 chunks\n",
      "‚úÖ 4200 records ‚Üí 45595 chunks\n",
      "‚úÖ 4300 records ‚Üí 46674 chunks\n",
      "‚úÖ 4400 records ‚Üí 47572 chunks\n",
      "‚úÖ 4500 records ‚Üí 48870 chunks\n",
      "‚úÖ 4600 records ‚Üí 49972 chunks\n",
      "‚úÖ 4700 records ‚Üí 51044 chunks\n",
      "‚úÖ 4800 records ‚Üí 52255 chunks\n",
      "üìñ ƒê√£ ƒë·ªçc 21800000 k√Ω t·ª±...\n",
      "‚úÖ 4900 records ‚Üí 53398 chunks\n",
      "‚úÖ 5000 records ‚Üí 54524 chunks\n",
      "‚úÖ 5100 records ‚Üí 55606 chunks\n",
      "‚úÖ 5200 records ‚Üí 56585 chunks\n",
      "‚úÖ 5300 records ‚Üí 57716 chunks\n",
      "‚úÖ 5400 records ‚Üí 58659 chunks\n",
      "‚úÖ 5500 records ‚Üí 59688 chunks\n",
      "‚úÖ 5600 records ‚Üí 60907 chunks\n",
      "‚úÖ 5700 records ‚Üí 62389 chunks\n",
      "‚úÖ 5800 records ‚Üí 63572 chunks\n",
      "‚úÖ 5900 records ‚Üí 64491 chunks\n",
      "‚úÖ 6000 records ‚Üí 65818 chunks\n",
      "‚úÖ 6100 records ‚Üí 66925 chunks\n",
      "‚úÖ 6200 records ‚Üí 68082 chunks\n",
      "‚úÖ 6300 records ‚Üí 69284 chunks\n",
      "‚úÖ 6400 records ‚Üí 70330 chunks\n",
      "‚úÖ 6500 records ‚Üí 71378 chunks\n",
      "‚úÖ 6600 records ‚Üí 72384 chunks\n",
      "üìñ ƒê√£ ƒë·ªçc 30100000 k√Ω t·ª±...\n",
      "‚úÖ 6700 records ‚Üí 73481 chunks\n",
      "üìñ ƒê√£ ƒë·ªçc 30300000 k√Ω t·ª±...\n",
      "‚úÖ 6800 records ‚Üí 74529 chunks\n",
      "‚úÖ 6900 records ‚Üí 75615 chunks\n",
      "‚úÖ 7000 records ‚Üí 76607 chunks\n",
      "‚úÖ 7100 records ‚Üí 77816 chunks\n",
      "üìñ ƒê√£ ƒë·ªçc 32200000 k√Ω t·ª±...\n",
      "‚úÖ 7200 records ‚Üí 78803 chunks\n",
      "‚úÖ 7300 records ‚Üí 80204 chunks\n",
      "‚úÖ 7400 records ‚Üí 81088 chunks\n",
      "‚úÖ 7500 records ‚Üí 82014 chunks\n",
      "‚úÖ 7600 records ‚Üí 83010 chunks\n",
      "‚úÖ 7700 records ‚Üí 83963 chunks\n",
      "‚úÖ 7800 records ‚Üí 85092 chunks\n",
      "üìñ ƒê√£ ƒë·ªçc 35300000 k√Ω t·ª±...\n",
      "‚úÖ 7900 records ‚Üí 86140 chunks\n",
      "‚úÖ 8000 records ‚Üí 87262 chunks\n",
      "‚úÖ 8100 records ‚Üí 88376 chunks\n",
      "‚úÖ 8200 records ‚Üí 89464 chunks\n",
      "‚úÖ 8300 records ‚Üí 90324 chunks\n",
      "‚úÖ 8400 records ‚Üí 91393 chunks\n",
      "‚úÖ 8500 records ‚Üí 92622 chunks\n",
      "üìñ ƒê√£ ƒë·ªçc 38100000 k√Ω t·ª±...\n",
      "‚úÖ 8600 records ‚Üí 94149 chunks\n",
      "‚úÖ 8700 records ‚Üí 95177 chunks\n",
      "‚úÖ 8800 records ‚Üí 96218 chunks\n",
      "‚úÖ 8900 records ‚Üí 97259 chunks\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 3) H√ÄM TI·ªÄN X·ª¨ L√ù & CHUNKING (√≠t RAM)\n",
    "# =========================\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "from typing import Generator, Dict, Any, List\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "\n",
    "FIELD_ORDER = [\n",
    "    \"ten_thu_tuc\", \"yeu_cau_dieu_kien\", \"thanh_phan_ho_so\",\n",
    "    \"trinh_tu_thuc_hien\", \"cach_thuc_thuc_hien\",\n",
    "    \"co_quan_thuc_hien\", \"thu_tuc_lien_quan\"\n",
    "]\n",
    "\n",
    "FIELD_VN_NAME = {\n",
    "    \"ten_thu_tuc\": \"T√™n th·ªß t·ª•c\",\n",
    "    \"yeu_cau_dieu_kien\": \"Y√™u c·∫ßu, ƒëi·ªÅu ki·ªán\",\n",
    "    \"thanh_phan_ho_so\": \"Th√†nh ph·∫ßn h·ªì s∆°\",\n",
    "    \"trinh_tu_thuc_hien\": \"Tr√¨nh t·ª± th·ª±c hi·ªán\",\n",
    "    \"cach_thuc_thuc_hien\": \"C√°ch th·ª©c th·ª±c hi·ªán\",\n",
    "    \"co_quan_thuc_hien\": \"C∆° quan th·ª±c hi·ªán\",\n",
    "    \"thu_tuc_lien_quan\": \"Th·ªß t·ª•c li√™n quan\"\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Chu·∫©n h√≥a text\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = str(s).replace(\"\\r\\n\", \"\\n\").replace(\"\\t\", \" \").strip()\n",
    "    s = re.sub(r\"[ \\u00A0]+\", \" \", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s\n",
    "\n",
    "def split_text_rcts(\n",
    "    text: str,\n",
    "    max_chars: int = 800,\n",
    "    overlap: int = 50,\n",
    "    separators: List[str] = None\n",
    ") -> Generator[str, None, None]:\n",
    "    \"\"\"Recursive Text Character Split (RCTS)\"\"\"\n",
    "    text = normalize_text(text)\n",
    "    if not text:\n",
    "        return\n",
    "\n",
    "    if separators is None:\n",
    "        separators = [\"\\n\\n\", \"\\n\", \". \", \", \", \" \"]\n",
    "\n",
    "    def _split_recursive(txt: str) -> List[str]:\n",
    "        if len(txt) <= max_chars:\n",
    "            return [txt]\n",
    "\n",
    "        # t√¨m separator ph√π h·ª£p nh·∫•t\n",
    "        for sep in separators:\n",
    "            idx = txt.rfind(sep, 0, max_chars)\n",
    "            if idx != -1 and idx > max_chars * 0.3:  # tr√°nh c·∫Øt qu√° s·ªõm\n",
    "                left = txt[:idx + len(sep)].strip()\n",
    "                right = txt[idx + len(sep):].strip()\n",
    "                return _split_recursive(left) + _split_recursive(right)\n",
    "\n",
    "        # n·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c -> c·∫Øt c·ª©ng\n",
    "        left = txt[:max_chars]\n",
    "        right = txt[max_chars:]\n",
    "        return [left] + _split_recursive(right)\n",
    "\n",
    "    chunks = _split_recursive(text)\n",
    "\n",
    "    # th√™m overlap gi·ªØa c√°c chunk\n",
    "    final_chunks = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i > 0 and overlap > 0:\n",
    "            prefix = chunks[i-1][-overlap:]\n",
    "            chunk = prefix + \" \" + chunk\n",
    "        final_chunks.append(chunk.strip())\n",
    "\n",
    "    return (c for c in final_chunks if c.strip())\n",
    "\n",
    "def create_chunk(record: Dict[str, Any], field: str, piece: str, piece_idx: int) -> str:\n",
    "    \"\"\"T·∫°o m·ªôt chunk JSON\"\"\"\n",
    "    parent_id = str(record.get(\"nguon\", str(uuid.uuid4())[:8]))\n",
    "    title = normalize_text(record.get(\"ten_thu_tuc\", \"\"))\n",
    "    source = str(record.get(\"nguon\", \"\"))\n",
    "\n",
    "    field_name = FIELD_VN_NAME.get(field, field)\n",
    "    text_content = f\"Th·ªß t·ª•c: {title}\\nM·ª•c: {field_name}\\nN·ªôi dung: {piece}\"\n",
    "\n",
    "    chunk = {\n",
    "        \"id\": f\"{parent_id}#{field}#{piece_idx}\",\n",
    "        \"parent_id\": parent_id,\n",
    "        \"ten_thu_tuc\": title,\n",
    "        \"field\": field,\n",
    "        \"text\": text_content,\n",
    "        \"raw\": piece,\n",
    "        \"nguon\": source\n",
    "    }\n",
    "\n",
    "    return json.dumps(chunk, ensure_ascii=False)\n",
    "\n",
    "def process_record_streaming(record: Dict[str, Any], outfile) -> int:\n",
    "    \"\"\"X·ª≠ l√Ω m·ªôt record v√† ghi tr·ª±c ti·∫øp ra file\"\"\"\n",
    "    chunk_count = 0\n",
    "\n",
    "    for field in FIELD_ORDER:\n",
    "        raw_value = record.get(field)\n",
    "        if not raw_value:\n",
    "            continue\n",
    "\n",
    "        piece_idx = 0\n",
    "        for piece in split_text_rcts(str(raw_value)):\n",
    "            if not piece.strip():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                chunk_json = create_chunk(record, field, piece, piece_idx)\n",
    "                outfile.write(chunk_json + \"\\n\")\n",
    "                chunk_count += 1\n",
    "                piece_idx += 1\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå L·ªói t·∫°o chunk cho field {field}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return chunk_count\n",
    "\n",
    "def safe_json_stream_parser(filepath: str) -> Generator[Dict[str, Any], None, None]:\n",
    "    \"\"\"Parser JSON streaming an to√†n h∆°n\"\"\"\n",
    "\n",
    "    print(\"üîÑ ƒêang parse JSON stream...\")\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8', buffering=8192) as f:\n",
    "            # ƒê·ªçc v√† ki·ªÉm tra k√Ω t·ª± ƒë·∫ßu\n",
    "            first_char = f.read(1)\n",
    "            if first_char != '[':\n",
    "                print(f\"‚ùå File kh√¥ng b·∫Øt ƒë·∫ßu b·∫±ng '[', m√† l√† '{first_char}'\")\n",
    "                return\n",
    "\n",
    "            # Reset v·ªÅ ƒë·∫ßu v√† b·ªè qua '['\n",
    "            f.seek(1)\n",
    "\n",
    "            buffer = \"\"\n",
    "            brace_count = 0\n",
    "            bracket_count = 0\n",
    "            in_string = False\n",
    "            escape_next = False\n",
    "            char_count = 0\n",
    "\n",
    "            while True:\n",
    "                char = f.read(1)\n",
    "                if not char:\n",
    "                    break\n",
    "\n",
    "                char_count += 1\n",
    "\n",
    "                # X·ª≠ l√Ω escape characters\n",
    "                if escape_next:\n",
    "                    buffer += char\n",
    "                    escape_next = False\n",
    "                    continue\n",
    "\n",
    "                if char == '\\\\' and in_string:\n",
    "                    buffer += char\n",
    "                    escape_next = True\n",
    "                    continue\n",
    "\n",
    "                # X·ª≠ l√Ω strings\n",
    "                if char == '\"':\n",
    "                    in_string = not in_string\n",
    "                    buffer += char\n",
    "                    continue\n",
    "\n",
    "                if in_string:\n",
    "                    buffer += char\n",
    "                    continue\n",
    "\n",
    "                # X·ª≠ l√Ω c·∫•u tr√∫c JSON\n",
    "                if char == '{':\n",
    "                    brace_count += 1\n",
    "                    buffer += char\n",
    "                elif char == '}':\n",
    "                    brace_count -= 1\n",
    "                    buffer += char\n",
    "\n",
    "                    # Ho√†n th√†nh m·ªôt object\n",
    "                    if brace_count == 0 and bracket_count == 0:\n",
    "                        try:\n",
    "                            # Clean buffer tr∆∞·ªõc khi parse\n",
    "                            clean_buffer = buffer.strip().rstrip(',')\n",
    "                            if clean_buffer:\n",
    "                                obj = json.loads(clean_buffer)\n",
    "                                yield obj\n",
    "                                del obj\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"‚ùå JSON decode error t·∫°i k√Ω t·ª± {char_count}: {e}\")\n",
    "                            print(f\"   Buffer: {buffer[:100]}...\")\n",
    "\n",
    "                        # Reset buffer\n",
    "                        buffer = \"\"\n",
    "                        gc.collect()\n",
    "\n",
    "                elif char == '[':\n",
    "                    bracket_count += 1\n",
    "                    buffer += char\n",
    "                elif char == ']':\n",
    "                    if bracket_count > 0:\n",
    "                        bracket_count -= 1\n",
    "                        buffer += char\n",
    "                    else:\n",
    "                        # K·∫øt th√∫c main array\n",
    "                        break\n",
    "                elif char == ',':\n",
    "                    if brace_count == 0 and bracket_count == 0:\n",
    "                        # D·∫•u ph·∫©y gi·ªØa c√°c objects ch√≠nh\n",
    "                        continue\n",
    "                    else:\n",
    "                        buffer += char\n",
    "                elif char in ' \\t\\n\\r':\n",
    "                    # Whitespace\n",
    "                    if buffer.strip():  # Ch·ªâ th√™m n·∫øu buffer kh√¥ng r·ªóng\n",
    "                        buffer += ' '\n",
    "                else:\n",
    "                    buffer += char\n",
    "\n",
    "                # Progress indicator\n",
    "                if char_count % 100000 == 0:\n",
    "                    print(f\"üìñ ƒê√£ ƒë·ªçc {char_count} k√Ω t·ª±...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói ƒë·ªçc file: {e}\")\n",
    "\n",
    "def process_one_record(record: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"X·ª≠ l√Ω 1 record v√† tr·∫£ v·ªÅ danh s√°ch JSON string (chunk).\"\"\"\n",
    "    chunks = []\n",
    "    for field in FIELD_ORDER:\n",
    "        raw_value = record.get(field)\n",
    "        if not raw_value:\n",
    "            continue\n",
    "\n",
    "        piece_idx = 0\n",
    "        for piece in split_text_rcts(str(raw_value)):\n",
    "            if not piece.strip():\n",
    "                continue\n",
    "            try:\n",
    "                chunk_json = create_chunk(record, field, piece, piece_idx)\n",
    "                chunks.append(chunk_json)\n",
    "                piece_idx += 1\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå L·ªói t·∫°o chunk cho field {field}: {e}\")\n",
    "                continue\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunking_main():\n",
    "    \"\"\"Chunking ch√≠nh ‚Äì ch·∫°y song song b·∫±ng to√†n b·ªô CPU core\"\"\"\n",
    "    print(\"=== B·∫ÆT ƒê·∫¶U CHUNKING (MULTICORE) ===\")\n",
    "\n",
    "    if not os.path.exists(RAW_JSON_PATH):\n",
    "        print(f\"‚ùå File kh√¥ng t·ªìn t·∫°i: {RAW_JSON_PATH}\")\n",
    "        return\n",
    "\n",
    "    file_size_mb = os.path.getsize(RAW_JSON_PATH) / (1024 * 1024)\n",
    "    print(f\"üìÅ K√≠ch th∆∞·ªõc file: {file_size_mb:.2f} MB\")\n",
    "\n",
    "    total_chunks = 0\n",
    "    processed_records = 0\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    print(f\"üñ•Ô∏è Ph√°t hi·ªán {num_cores} CPU core ‚Äì d√πng t·ªëi ƒëa\")\n",
    "\n",
    "    try:\n",
    "        with open(CHUNKS_JSONL, 'w', encoding='utf-8', buffering=1024) as outfile, \\\n",
    "             ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "\n",
    "            futures = []\n",
    "            for record in safe_json_stream_parser(RAW_JSON_PATH):\n",
    "                futures.append(executor.submit(process_one_record, record))\n",
    "                processed_records += 1\n",
    "\n",
    "                # ƒë·ªÉ tr√°nh d·ªìn qu√° nhi·ªÅu future -> flush d·∫ßn\n",
    "                if len(futures) >= 100:\n",
    "                    for f in as_completed(futures):\n",
    "                        chunks = f.result()\n",
    "                        for c in chunks:\n",
    "                            outfile.write(c + \"\\n\")\n",
    "                        total_chunks += len(chunks)\n",
    "                    outfile.flush()\n",
    "                    futures = []\n",
    "                    gc.collect()\n",
    "\n",
    "                    if processed_records % 50 == 0:\n",
    "                        print(f\"‚úÖ {processed_records} records ‚Üí {total_chunks} chunks\")\n",
    "\n",
    "            # X·ª≠ l√Ω n·ªët c√°c future c√≤n l·∫°i\n",
    "            for f in as_completed(futures):\n",
    "                chunks = f.result()\n",
    "                for c in chunks:\n",
    "                    outfile.write(c + \"\\n\")\n",
    "                total_chunks += len(chunks)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói fatal: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nüéâ HO√ÄN TH√ÄNH!\")\n",
    "    print(f\"üìä T·ªïng k·∫øt: {processed_records} records ‚Üí {total_chunks} chunks\")\n",
    "    print(f\"üíæ Output: {CHUNKS_JSONL}\")\n",
    "\n",
    "def fallback_simple_load():\n",
    "    \"\"\"Ph∆∞∆°ng √°n d·ª± ph√≤ng: load tr·ª±c ti·∫øp (cho file nh·ªè)\"\"\"\n",
    "    print(\"üîÑ Th·ª≠ ph∆∞∆°ng √°n load tr·ª±c ti·∫øp...\")\n",
    "\n",
    "    try:\n",
    "        with open(RAW_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        print(f\"‚úÖ Load th√†nh c√¥ng! T·ªïng records: {len(data)}\")\n",
    "\n",
    "        total_chunks = 0\n",
    "\n",
    "        with open(CHUNKS_JSONL, 'w', encoding='utf-8') as outfile:\n",
    "            for i, record in enumerate(data):\n",
    "                try:\n",
    "                    chunks_created = process_record_streaming(record, outfile)\n",
    "                    total_chunks += chunks_created\n",
    "\n",
    "                    if (i + 1) % 100 == 0:\n",
    "                        print(f\"‚úÖ {i + 1}/{len(data)} ‚Üí {total_chunks} chunks\")\n",
    "                        outfile.flush()\n",
    "                        gc.collect()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå L·ªói record {i}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        print(f\"üéâ Fallback ho√†n th√†nh: {len(data)} records ‚Üí {total_chunks} chunks\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fallback th·∫•t b·∫°i: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"=== ROBUST JSON CHUNKING ===\")\n",
    "\n",
    "    try:\n",
    "        chunking_main()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ph∆∞∆°ng ph√°p ch√≠nh th·∫•t b·∫°i: {e}\")\n",
    "        print(\"üîÑ Th·ª≠ ph∆∞∆°ng √°n d·ª± phong...\")\n",
    "        fallback_simple_load()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3C5CbayMMRMt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMS1oF70FgHr6XaJNKtkE6y",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
