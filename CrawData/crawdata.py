# -*- coding: utf-8 -*-
import time
import json
import os
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# ==============================================================================
# PH·∫¶N 1: C√ÅC H√ÄM CH·ª®C NƒÇNG
# ==============================================================================

def setup_driver():
    """
    H√†m n√†y thi·∫øt l·∫≠p v√† tr·∫£ v·ªÅ m·ªôt ƒë·ªëi t∆∞·ª£ng WebDriver c·ªßa Selenium.
    - S·ª≠ d·ª•ng webdriver-manager ƒë·ªÉ t·ª± ƒë·ªông t·∫£i v√† qu·∫£n l√Ω chromedriver.
    - Ch·∫°y ·ªü ch·∫ø ƒë·ªô kh√¥ng giao di·ªán (headless) ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô v√† kh√¥ng l√†m phi·ªÅn ng∆∞·ªùi d√πng.
    """
    print("... _ƒêang kh·ªüi t·∫°o tr√¨nh duy·ªát Chrome_ ...")
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')  # Ch·∫°y ·∫©n, kh√¥ng m·ªü c·ª≠a s·ªï tr√¨nh duy·ªát
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('log-level=3') # Ch·ªâ hi·ªÉn th·ªã l·ªói nghi√™m tr·ªçng
    options.add_argument("window-size=1920,1080") # ƒê·∫∑t k√≠ch th∆∞·ªõc c·ª≠a s·ªï ƒë·ªÉ tr√°nh l·ªói responsive

    # S·ª≠ d·ª•ng ChromeDriverManager ƒë·ªÉ t·ª± ƒë·ªông c√†i ƒë·∫∑t v√† cung c·∫•p ƒë∆∞·ªùng d·∫´n chromedriver
    service = ChromeService(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    print("_Tr√¨nh duy·ªát ƒë√£ s·∫µn s√†ng!_")
    return driver

def scrape_procedure_details(page_source, url):
    """
    Ph√¢n t√≠ch m√£ HTML c·ªßa trang chi ti·∫øt th·ªß t·ª•c ƒë·ªÉ tr√≠ch xu·∫•t th√¥ng tin.
    
    Args:
        page_source (str): M√£ ngu·ªìn HTML c·ªßa trang.
        url (str): URL c·ªßa trang ƒëang ƒë∆∞·ª£c ph√¢n t√≠ch, ƒë·ªÉ l∆∞u l·∫°i ngu·ªìn.

    Returns:
        dict: M·ªôt dictionary ch·ª©a c√°c th√¥ng tin chi ti·∫øt c·ªßa th·ªß t·ª•c.
              Tr·∫£ v·ªÅ None n·∫øu c√≥ l·ªói x·∫£y ra.
    """
    try:
        soup = BeautifulSoup(page_source, 'html.parser')

        # Kh·ªüi t·∫°o c√°c bi·∫øn v·ªõi gi√° tr·ªã m·∫∑c ƒë·ªãnh
        defaults = {
            'ten_thu_tuc': "Kh√¥ng t√¨m th·∫•y", 'cach_thuc_thuc_hien': "Kh√¥ng t√¨m th·∫•y",
            'thanh_phan_ho_so': "Kh√¥ng t√¨m th·∫•y", 'trinh_tu_thuc_hien': "Kh√¥ng t√¨m th·∫•y",
            'co_quan_thuc_hien': "Kh√¥ng t√¨m th·∫•y", 'yeu_cau_dieu_kien': "Kh√¥ng t√¨m th·∫•y",
            'thu_tuc_lien_quan': "Kh√¥ng t√¨m th·∫•y"
        }

        # 1. L·∫•y t√™n th·ªß t·ª•c
        title_tag = soup.find('h1')
        if title_tag:
            defaults['ten_thu_tuc'] = title_tag.get_text(strip=True)

        # 2. L·∫•y "C√°ch th·ª©c th·ª±c hi·ªán"
        all_tables = soup.find_all('table', class_='table-data')
        for table in all_tables:
            header = table.find('thead')
            if header and 'h√¨nh th·ª©c n·ªôp' in header.get_text(strip=True).lower():
                rows = []
                for row in table.find('tbody').find_all('tr'):
                    cells = [cell.get_text(strip=True, separator=' ') for cell in row.find_all('td')]
                    if len(cells) >= 4:
                        rows.append(f"- H√¨nh th·ª©c: {cells[0]}. Th·ªùi h·∫°n: {cells[1]}. Ph√≠: {cells[2]}. M√¥ t·∫£: {cells[3]}")
                if rows:
                    defaults['cach_thuc_thuc_hien'] = "\n".join(rows)
                break

        # 3. L·∫•y "Th√†nh ph·∫ßn h·ªì s∆°"
        ho_so_header = soup.find('h2', string=lambda text: text and 'th√†nh ph·∫ßn h·ªì s∆°' in text.lower())
        if ho_so_header and ho_so_header.find_next_sibling('div'):
            rows = []
            for table_hs in ho_so_header.find_next_sibling('div').find_all('table', class_='table-data'):
                for row in table_hs.find('tbody').find_all('tr'):
                    first_cell = row.find('td', class_='justify')
                    if first_cell:
                        rows.append(first_cell.get_text(strip=True))
            if rows:
                defaults['thanh_phan_ho_so'] = "\n".join(rows)

        # 4. L·∫•y "Tr√¨nh t·ª± th·ª±c hi·ªán"
        trinh_tu_header = soup.find('h2', string=lambda text: text and 'tr√¨nh t·ª± th·ª±c hi·ªán' in text.lower())
        if trinh_tu_header and trinh_tu_header.find_next_sibling('div'):
            defaults['trinh_tu_thuc_hien'] = trinh_tu_header.find_next_sibling('div').get_text(strip=True, separator='\n')

        # 5. L·∫•y "C∆° quan th·ª±c hi·ªán" v√† "Y√™u c·∫ßu, ƒëi·ªÅu ki·ªán"
        list_expand_divs = soup.find_all('div', class_='list-expand')
        for div in list_expand_divs:
            for item in div.find_all('div', class_='item'):
                title_tag = item.find('div', class_='title')
                content_tag = item.find('div', class_='content')
                if title_tag and content_tag:
                    title_text = title_tag.get_text(strip=True).lower()
                    content_text = content_tag.get_text(strip=True, separator='\n')
                    if 'c∆° quan th·ª±c hi·ªán' in title_text:
                        defaults['co_quan_thuc_hien'] = content_text
                    elif 'y√™u c·∫ßu, ƒëi·ªÅu ki·ªán' in title_text:
                        defaults['yeu_cau_dieu_kien'] = content_text
        
        # 6. L·∫•y "Th·ªß t·ª•c h√†nh ch√≠nh li√™n quan"
        lien_quan_header = soup.find('h2', string=lambda text: text and 'th·ªß t·ª•c h√†nh ch√≠nh li√™n quan' in text.lower())
        if lien_quan_header and lien_quan_header.find_next_sibling('ul'):
            lien_quan_ul = lien_quan_header.find_next_sibling('ul')
            related_items = [li.get_text(strip=True) for li in lien_quan_ul.find_all('li')]
            if related_items:
                defaults['thu_tuc_lien_quan'] = "\n".join(related_items)
            else: # Tr∆∞·ªùng h·ª£p kh√¥ng c√≥ th·∫ª li m√† ch·ªâ c√≥ text
                 defaults['thu_tuc_lien_quan'] = lien_quan_ul.get_text(strip=True)


        # Th√™m ngu·ªìn v√†o d·ªØ li·ªáu
        defaults['nguon'] = url
        return defaults
    
    except Exception as e:
        print(f"L·ªói khi ph√¢n t√≠ch HTML trang chi ti·∫øt: {url} - {e}")
        return None

def get_all_detail_links(driver, start_url):
    """
    Thu th·∫≠p t·∫•t c·∫£ c√°c link d·∫´n ƒë·∫øn trang chi ti·∫øt th·ªß t·ª•c t·ª´ m·ªôt URL b·∫Øt ƒë·∫ßu.
    H√†m n√†y s·∫Ω t·ª± ƒë·ªông chuy·ªÉn trang cho ƒë·∫øn khi h·∫øt.

    Args:
        driver: ƒê·ªëi t∆∞·ª£ng WebDriver c·ªßa Selenium.
        start_url (str): URL c·ªßa trang danh s√°ch th·ªß t·ª•c c·∫ßn c√†o.

    Returns:
        list: M·ªôt danh s√°ch c√°c URL chi ti·∫øt ƒë√£ ƒë∆∞·ª£c thu th·∫≠p.
    """
    all_detail_links = []
    wait = WebDriverWait(driver, 15)
    
    try:
        driver.get(start_url)
        wait.until(EC.presence_of_element_located((By.ID, "table-main")))
        print(f"   ƒê√£ m·ªü trang danh s√°ch: {start_url}")
    except TimeoutException:
        print(f"   L·ªói: Kh√¥ng th·ªÉ t·∫£i trang danh s√°ch trong th·ªùi gian ch·ªù. B·ªè qua URL n√†y.")
        return []

    page_count = 1
    while True:
        print(f"   --- ƒêang c√†o c√°c link ·ªü Trang {page_count} ---")
        try:
            # Ch·ªù cho ƒë·∫øn khi h√†ng ƒë·∫ßu ti√™n c·ªßa b·∫£ng xu·∫•t hi·ªán
            first_row_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#table-main tbody tr")))
        except TimeoutException:
            print("   Kh√¥ng t√¨m th·∫•y h√†ng d·ªØ li·ªáu n√†o trong b·∫£ng. C√≥ th·ªÉ trang n√†y kh√¥ng c√≥ d·ªØ li·ªáu.")
            break

        # L·∫•y link t·ª´ trang hi·ªán t·∫°i
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        base_url = 'https://thutuc.dichvucong.gov.vn/p/home/'
        main_table = soup.find('table', id='table-main')
        if main_table and main_table.find('tbody'):
            links_found_on_page = 0
            for row in main_table.find('tbody').find_all('tr'):
                a_tag = row.find('a')
                if a_tag and a_tag.get('href'):
                    relative_link = a_tag.get('href')
                    full_link = base_url + relative_link
                    if full_link not in all_detail_links:
                        all_detail_links.append(full_link)
                        links_found_on_page += 1
            print(f"   ƒê√£ t√¨m th·∫•y {links_found_on_page} link m·ªõi.")
        else:
             print("   Kh√¥ng t√¨m th·∫•y b·∫£ng d·ªØ li·ªáu tr√™n trang n√†y.")


        # C·ªë g·∫Øng chuy·ªÉn sang trang ti·∫øp theo
        try:
            # Cu·ªôn xu·ªëng khu v·ª±c ph√¢n trang ƒë·ªÉ ƒë·∫£m b·∫£o n√∫t 'Next' c√≥ th·ªÉ ƒë∆∞·ª£c click
            pagination_area = driver.find_element(By.ID, "paginationPanel")
            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", pagination_area)
            time.sleep(1) 

            # T√¨m v√† click n√∫t "Next"
            next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "li.next:not(.disabled) a")))
            driver.execute_script("arguments[0].click();", next_button)

            # Ch·ªù cho trang m·ªõi t·∫£i xong b·∫±ng c√°ch ki·ªÉm tra ph·∫ßn t·ª≠ c≈© ƒë√£ l·ªói th·ªùi
            wait.until(EC.staleness_of(first_row_element))
            page_count += 1
        except (NoSuchElementException, TimeoutException):
            print("   Kh√¥ng t√¨m th·∫•y n√∫t 'Next' ho·∫∑c ƒë√£ ƒë·∫øn trang cu·ªëi c√πng.")
            break

    print(f"   ƒê√£ thu th·∫≠p ƒë∆∞·ª£c t·ªïng c·ªông {len(all_detail_links)} link chi ti·∫øt.")
    return all_detail_links


# ==============================================================================
# PH·∫¶N 2: CH∆Ø∆†NG TR√åNH CH√çNH
# ==============================================================================
if __name__ == '__main__':
    # --- C·∫§U H√åNH ---
    # Danh s√°ch c√°c ngu·ªìn d·ªØ li·ªáu c·∫ßn c√†o
    TARGET_URLS = {
        "Cac_Bo_Nganh": 'https://thutuc.dichvucong.gov.vn/p/home/dvc-tthc-category.html?tinh_bo=0&tu_khoa=&co_quan_cong_bo=-1&cap_thuc_hien=-1&linh_vuc=-1&loai_tthc=-1&doi_tuong_thuc_hien=-1&is_advanced_search=0',
        "Da_Nang": 'https://thutuc.dichvucong.gov.vn/p/home/dvc-tthc-category.html?loai_tthc=0&co_quan_cong_bo=426100&tinh_bo=1&is_advanced_search=1',
        "TP_HCM": 'https://thutuc.dichvucong.gov.vn/p/home/dvc-tthc-category.html?loai_tthc=0&co_quan_cong_bo=411312&tinh_bo=1&is_advanced_search=1',
        "Ha_Noi": 'https://thutuc.dichvucong.gov.vn/p/home/dvc-tthc-category.html?loai_tthc=0&co_quan_cong_bo=389181&tinh_bo=1&is_advanced_search=1'
    }
    FINAL_OUTPUT_FILE = 'toan_bo_du_lieu_final.json'

    # --- KH·ªûI ƒê·ªòNG ---
    main_driver = setup_driver()
    wait = WebDriverWait(main_driver, 15)
    all_procedures_data = []
    
    # --- V√íNG L·∫∂P C√ÄO D·ªÆ LI·ªÜU T·ª™ C√ÅC NGU·ªíN ---
    for name, start_url in TARGET_URLS.items():
        print(f"\n========================================================")
        print(f"B·∫ÆT ƒê·∫¶U C√ÄO D·ªÆ LI·ªÜU C·ª¶A: {name}")
        print(f"========================================================")

        # Giai ƒëo·∫°n 1: L·∫•y t·∫•t c·∫£ c√°c link chi ti·∫øt
        detail_links = get_all_detail_links(main_driver, start_url)

        if not detail_links:
            print(f"Kh√¥ng c√≥ link n√†o ƒë·ªÉ c√†o cho {name}. B·ªè qua.")
            continue

        # Giai ƒëo·∫°n 2: L·∫∑p qua t·ª´ng link chi ti·∫øt ƒë·ªÉ c√†o d·ªØ li·ªáu
        print(f"\n--- B·∫Øt ƒë·∫ßu c√†o d·ªØ li·ªáu chi ti·∫øt cho {name} ---")
        for i, link in enumerate(detail_links):
            print(f"   [{i + 1}/{len(detail_links)}] ƒêang x·ª≠ l√Ω: {link}")
            try:
                main_driver.get(link)
                # Ch·ªù cho ti√™u ƒë·ªÅ (th·∫ª h1) c·ªßa trang chi ti·∫øt xu·∫•t hi·ªán
                wait.until(EC.presence_of_element_located((By.TAG_NAME, "h1")))
                
                # G·ªçi h√†m ph√¢n t√≠ch HTML
                data = scrape_procedure_details(main_driver.page_source, link)
                
                if data:
                    all_procedures_data.append(data)
                    # print(f"   ‚úÖ C√†o th√†nh c√¥ng: {data.get('ten_thu_tuc', 'N/A')[:70]}...")
                
                # T·∫°m d·ª´ng m·ªôt ch√∫t ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
                time.sleep(0.5) 
            except Exception as e:
                print(f"  L·ªói nghi√™m tr·ªçng khi x·ª≠ l√Ω link {link}: {e}")
                # C√≥ th·ªÉ l∆∞u link l·ªói v√†o file ri√™ng n·∫øu c·∫ßn
                with open('error_links.txt', 'a', encoding='utf-8') as f_err:
                    f_err.write(f"{link}\n")
                continue

        print(f"- Ho√†n t·∫•t c√†o d·ªØ li·ªáu cho: {name}")

    # --- D·ªåN D·∫∏P V√Ä L∆ØU K·∫æT QU·∫¢ ---
    print("\n========================================================")
    print("üßπ ƒê√≥ng tr√¨nh duy·ªát...")
    main_driver.quit()

    print(f"ƒêang l∆∞u t·∫•t c·∫£ d·ªØ li·ªáu v√†o file '{FINAL_OUTPUT_FILE}'...")
    try:
        with open(FINAL_OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(all_procedures_data, f, indent=4, ensure_ascii=False)
        print(f"\nHO√ÄN T·∫§T!")
        print(f"ƒê√£ l∆∞u th√†nh c√¥ng d·ªØ li·ªáu c·ªßa {len(all_procedures_data)} th·ªß t·ª•c.")
        print(f"File k·∫øt qu·∫£: {os.path.abspath(FINAL_OUTPUT_FILE)}")
    except Exception as e:
        print(f"L·ªói khi ghi file: {e}")